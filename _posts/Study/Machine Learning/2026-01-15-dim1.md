---
# title: Dimensionality Reduction & Matrix Factorization
title: Dimensionality
description: Summary of Dimensionality Reduction & Matrix Factorization
date: 2026-01-15 00:00:00
categories: [Study, Machine Learning ]
author: PythonToGo
tags: [Machine Learning, Unsupervised Learning, PCA, SVD, t-SNE, Autoencoder, ]
# pin: true
math: true
mermaid: true
comments: true
image: 
#     # type: pdf
    path: assets/img/posts/study-ai/ch7-re-proof.png

#     # page: 7
#     alt: <div class="pdf-viewer-container" data-pdf="/assets/img/posts/study-ai/ch7-LogicalAgents.pdf" data-page="2"></div>
---
{% include pdf-viewer.html %}


# Dimensionality Reduction & Matrix Factorization

> 차원 축소 Dimensionality Reduction 은 단순히 데이터를 줄이는 것이 아니라, **hidden correlations** 나 **topics** 를 발견하고 해석가능한 형태로 가공하는 강력한 도구이다. PCA/SVD 와 같은 Linear Method 부터, t-SNE/Autoencoder와 같은 non-linear method까지, 데이터의 특성과 목적에 맞는 모델선택이 중요하다.

## Introduction: Unsupervised Learning and Dimension Reduction

**Unsupervised Learning**은 데이터 내부에 숨겨진 구조(**latent structure**) 를 찾는 과정으로 ,데이터 포인트 $$x$$ 를 모델링하는 $$p(x)$$ 의 관점에서 이해할 수 있다. 


> ### Motivation of Dim Reduction 
>
> - **Curse of Dimensionality** : 차원이 높아질수록 밀도를 규명하기 위해 기하급수적으로 많은 데이터가 필요하다.
> - **Computational Efficiency** : 거리함수 계산복잡도를 낮추고 메모리를 절약한다.
> - **Visualization** : 고차원데이터는 시각화하기 어렵지만, 데이터가 대개 저차원 매니폴드(**low-dim. manifold)에 내장되어 있다는 점을 이용해 가시화할 수 있다.
{: .prompt-tip}


<div class="pdf-viewer-container" data-pdf="/assets/img/posts/study-ml/ch10.pdf" data-page="3"></div>

### Linear Transformations

원본 데이터 $$X$$ 를 직교변환행렬 $$F$$ 를 통해 새로운 좌표계 $$X' = X \cdot F$$로 변환한다. 이때 평균 벡터를 $$\bar{x}$$ 라고 하면, 변환된 공간에서의 평균은

$$
\bar{x}' = \bar{x}\cdot F
$$

이며, 공분산 행렬은

$$
\Sigma_{x'} = F^{T}\cdot \Sigma_{x}\cdot F
$$



## Principal Component Analysis (PCA)

PCA 의 목적은 새로운 차원들 사이의 상관관계가 0이 되도록 좌표계를 변환해, Variance가 낮은 차원을 무시함으로써 데이터를 압축하는 것이다.


![PCA Goal](/assets/img/posts/study-ml/ch10-pca-goal.png)

### Given

- $$N \ d-$$ dimensional data points: $$\{ x_i \}^N_{i=1}, x_i \in \mathbb{R}^d \ \forall i \in \{1, ..., N\}$$ 

그럼 우리는 $$X \in \mathbb{R}^{N \times d}$$ 를 다음과 같이 나타낼 수 있다. 

$$
X = \begin{bmatrix}
x_{11} & \cdots & x_{1d} \\
\vdots & \ddots & \vdots \\
x_{N1} & \cdots & x_{Nd}
\end{bmatrix}
$$

$$\rightarrow$$ Row $$x_i = \{ x_{i1}, ...,  x_{id} \} \in \mathbb{R}^d$$ 는 $$i$$-th point 를 뜻하며,  column $$X_{:,j}$$ 는 $$j$$-th dimension 의 모든 값을 포함하는 벡터를 뜻한다.


> What this means in practice:
>
> - The covariance matrix of the transformed data becomes a diagonal matrix. All off-diagonal elements (covariances between different dimensions) are zero. 
> 
> - The new axes, called Principal Components, are ordered by the amount of variance they capture from the original data. The first principal component is the direction of maximum variance.
{: .prompt-tip}

### Method
**1. Center the data** : 평균 $$\bar{x} = \frac{1}{N} X^T \mathbb{1}_N $$을 구해서 데이터에서 뺀다. 

$$
\tilde{x}_{i} = x_i - \bar{x}
$$

![Center the data](/assets/img/posts/study-ml/ch10-step1.png)

> Statistics
>
> - **Zero order statistic** : number of points $$N$$
>
> - **First order statistic** : the mean of the $$N$$ points, the vector $$\bar{x} \in \mathbb{R}^d$$ :
> $$
> \bar{x} = \begin{bmatrix}
> \bar{x}_1 \\
> \vdots \\
> \bar{x}_d
> \end{bmatrix}
> = \frac{1}{N} \cdot X^T \cdot \mathbb{1}_N
> $$


**2. Compute Covariance Matrix** : Covariance $$S$$ (or $$Cov(\tilde{X}_j)$$) 를 계산한다.

$$
S = \frac{1}{N} \sum^N_{n=1} (x_n - \bar{x})(x_n - \bar{x})^T
$$


#### Determine the Principal Components
 
- Determine the *variance* $$Var(\tilde{X}_j)$$ for each dimension $$j \in \{1, ..., d \}$$

- Determine the *covariance* $$Cov(\tilde{X}_{j1}, \tilde{X}_{j2})$$ between dimensions $$j_1$$ and $$j_2, \ \forall j_1 \neq j_2 \in \{ 1, ... , d \}$$

> Statistics
>
> - **Second order statistic** : Variance and Covariance
>
> The variance within the $$j$$-th dimension in $X$ is:
>
> $$
> Var(X_j) = \frac{1}{N} \sum^N_{i=1} (x_{ij} - \bar{x}_j)^2 = \frac{1}{N} \cdot X^T_j X_j - {\bar{x}_j}^2
> $$
>
> The covariance between dimension $j_1$ and $j_2$ is:
>
> $$
> Cov(X_{j1}, X_{j2}) = \frac{1}{N} \sum^N_{i=1} (x_{i j_1} - \bar{x}_{j_1}) = \frac{1}{N} \cdot X^T_{j_1} X_{j_2} - \bar{x}_{j_1} \bar{x}_{j_2}
> $$
>
> For the set of points contained in $$X$$ the corresponding covariance matrix is defined as:
>
> $$
> \Sigma_{X} =
> \begin{bmatrix}
> Var(X_1) & Cov(X_1, X_2) & \cdots & Cov(X_1, X_d) \\
> Cov(X_2, X_1) & Var(X_2) & \cdots & Cov(X_2, X_d) \\
> \vdots & \vdots & \ddots & \vdots \\
> Cov(X_d, X_1) & Cov(X_d, X_2) & \cdots & Var(X_d)
> \end{bmatrix}
> =
> \frac{1}{N} X^{T} X - \bar{x}\bar{x}^{T}
> $$


**3. SVD** : 공분산 행렬을 다음과 같이 분해한다. 

$$
\Sigma_{\tilde{X}} = \Gamma \cdot \Lambda \cdot \Gamma^T
$$

| Symbol | Description |
|---|---|
| $$\Gamma$$ | Eigenvectors Matrix 이고, 이들이 곧 **Principal components** 이다|
| $$\Lambda$$ | Eigenvalues ($$\lambda|i$$)를 대각성분으로 가지며, 이는 새로운 차원에서의 분산을 의미한다 |


### The result
- **Dimensionality Reduction** : 고유값이 큰 순서대로 $$k$$ 개의 Eigenvector 만 유지해서 $$Y_{\text{reduced}} = \tilde{X} \cdot \Gamma_{\text{truncated}}$$ 를 얻는다. 보통 전체 Variance 90% 이상을 설명할 수 있ㄴ슨 $$k$$ 를 선택한다. 

- **Variance** : 분산을 최대화하는 방향을  찾는 것 (=Induction proof 과정)은 결과적으로 *reconstruction error* 를 최소화하는 것과 동일하다. 


![PCA Algorithm](/assets/img/posts/study-ml/ch10-pca-algo.png)

---

![The Duality of PCA](/assets/img/posts/study-ml/ch10-duality.png)

---


### Dimensionality Reduction with PCA

#### Approach
- The coordinates with low variance (hence low $$\lambda_i$$) can be ignored
- W.l.o.g. let us assume $$\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_d$$

#### Truncation of $$\Gamma$$
- Keep only the first $$k$$ columns (eigenvectors) of $$\Gamma$$ corresponding to the largest $$k$$ eigenvalues $$\lambda_1,\ldots,\lambda_k$$.
- Denote $$\Gamma_k = [\gamma_1,\ldots,\gamma_k] \in \mathbb{R}^{d\times k}$$, then the reduced representation is $$Y = \tilde{X}\Gamma_k$$.

#### How to pick k?
- Frequently used: **90% rule** (explained variance ratio).
- Choose the smallest $$k$$ such that
  $$
  \frac{\sum_{i=1}^{k}\lambda_i}{\sum_{i=1}^{d}\lambda_i} \ge 0.9.
  $$

#### Result
- The modified points (transformed and truncated) retain most of the information of the original points while being low-dimensional.



> #### Complexity
>
> $$ O(N \cdot d^2) + O(d^3) + O(N \cdot d \cdot k) = O(N \cdot d^2 + d^3) $$
> compute covariance matrix + eigenvalue decomposition + project data onto the k-dim space
{: .prompt-info}


---

## Singular Value Decomposition (SVD)

SVD 는 임의의 행렬 A 를 세 행렬의 곱으로 분해하는 것.

$$
A = U \cdot \Sig \cdot V^T
$$


| Components | Description |
| --- | --- |
| $$U \in \math{R}^{n \times r} $$| Left singular vectors, User-to-concept 유사도 |
| $\Sig \in  \math{R}^{r \times r} $$ | Singular values($$\sig_i$$) 를 가진 대각행렬, concept's strength |
| $V \in  \math{R}^{d \times r} $$ | Right singular vectors, Movie-to-concept 유사도 |


### Low Rank Approximation :

