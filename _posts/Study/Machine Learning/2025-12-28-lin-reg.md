---
title: Linear Regression
description: Summary of linear regression, Regularization, Bias-Variance TradeOff
date: 2025-12-28
categories: [Study, Machine Learning]
tags: [Machine Learning, Linear Regression, Regularization]
# pin: true
math: true
mermaid: true
comments: true
image: 
    path: /assets/img/posts/study-ml/overfitting.png
    lqip: 
    alt: Overfitting example showing 10th degree polynomial overfitting on 3rd degree polynomial data 
---

# Linear Regression

## Basic Concept

Linear Regressionì˜ ëª©ì ì€ ì…ë ¥ê°’ $$x$$ì™€ íƒ€ê²Ÿ $$y$$ ì‚¬ì´ì˜ ë§¤í•‘ í•¨ìˆ˜ $$f(\cdot)$$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.

### Model Assumption

íƒ€ê²Ÿ $$y$$ëŠ” ê²°ì •ë¡ ì  í•¨ìˆ˜ $$f(x)$$ì— Noise $$\epsilon$$ì´ ë”í•´ì§„ í˜•íƒœì´ê³ , $$\epsilon$$ì€ í‰ê· ì´ 0ì¸ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤. ì¦‰ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´:

$$
y_i = f(x_i) + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \beta^{-1})
$$

### Linear Model

$$f(x)$$ê°€ íŒŒë¼ë¯¸í„° $$w$$ì— ëŒ€í•´ ì„ í˜•ì¸ ëª¨ë¸ì´ë‹¤. Bias term $$w_0$$ì„ $$w$$ ë²¡í„°ì— í¬í•¨í•˜ê³ , ì…ë ¥ $$x$$ ì•ì— 1ì„ ì¶”ê°€í•´ì„œ **Absorbing the bias**ë¥¼ ìˆ˜í–‰í•˜ë©´, $$f_w(x) = w^T x$$ë¡œ ê°„ê²°í•˜ê²Œ í‘œí˜„ëœë‹¤.

### Least Squares (LS) Loss

ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ ê´€ì¸¡ê°’ê³¼ ì˜ˆì¸¡ê°’ ì°¨ì´ì˜ ì œê³±í•©ì„ ì‚¬ìš©í•œë‹¤. ì´ê²ƒì´ ê·¸ ìœ ëª…í•œ **ì†ì‹¤ í•¨ìˆ˜**:

$$
E_{LS}(w) = \frac{1}{2} \sum_{i=1}^N (w^T x_i - y_i)^2
$$

### Normal Equation

ìš°ë¦¬ëŠ” ì†ì‹¤ì„ ìµœì†Œí™”í•´ì•¼ í•œë‹¤. ì†ì‹¤ í•¨ìˆ˜ $$E_{LS}(w)$$ì— ëŒ€í•œ ìµœì ì˜ ê°’ì„ $$w^{*}$$ë¼ê³  í•˜ê³ , ì´ê²ƒì€ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•´ì„œ êµ¬í•œë‹¤ (~í¸ë¯¸ë¶„ ëŠë‚Œ):

$$
w^{*} = (X^T X)^{-1} X^T y
$$

ì—¬ê¸°ì„œ $$(X^T X)^{-1} X^T$$ëŠ” **Moore-Penrose pseudo-inverse** $$X^{\dagger}$$ë¼ê³  ë¶€ë¥¸ë‹¤.



## Nonlinear Dependency & Basis Functions

ê·¸ë ‡ë‹¤ë©´ ë°ì´í„°ê°€ ë¹„ì„ í˜•ì ì´ë©´ ì–´ë–»ê²Œ ë ê¹Œ? ì¼ë‹¨ ë­ê°€ ëë“  ì„ í˜•ì´ ë¹„ì„ í˜•ë³´ë‹¤ calculationì´ ì‰½ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì–´ë–¤ í•¨ìˆ˜ë“  ìµœëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¡œ transformí•˜ëŠ” ì „ëµì„ ì„ íƒí•´ì•¼ í•œë‹¤.

Input $$x$$ë¥¼ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” **Basis functions** $$\phi(x)$$ë¥¼ ì‚¬ìš©í•´ì„œ ì„ í˜• ëª¨ë¸ì˜ í‹€ì„ ìœ ì§€í•˜ë©´ì„œ ë¹„ì„ í˜• ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. (ì•½ê°„ ì»¤ë„ ëŠë‚Œ)

### Prediction

$$
f_w(x) = w^T \phi(x)
$$

### Design Matrix $$\Phi$$

ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•œ ê¸°ì € í•¨ìˆ˜ì˜ ê°’ì„ í–‰ìœ¼ë¡œ ìŒ“ì€ í–‰ë ¬ì´ë‹¤.

### Solution

ìµœì í•´ëŠ” ê¸°ë³¸ ëª¨ë¸ê³¼ ë™ì¼í•œ í˜•íƒœì´ë‹¤:

$$
w^{*} = (\Phi^T \Phi)^{-1} \Phi^T y
$$

(í˜•íƒœê°€ ê°™ìœ¼ë‹ˆ ì„ í˜•ì²˜ëŸ¼ ë‹¤ë£¨ë©´ ëœë‹¤.)

### Transformation Invariance

ë§Œì•½ ì…ë ¥ì„ ì„ í˜• ë³€í™˜í•˜ë”ë¼ë„ ($$\phi(x_i) = A^T x_i$$, ì—¬ê¸°ì„œ $$A$$ëŠ” Full rank), ìµœì ì˜ íŒŒë¼ë¯¸í„° $$v^{*}$$ê°€ $$A^{-1}w^{*}$$ê°€ ë˜ì–´ ê²°êµ­ ë™ì¼í•œ prediction function $$f(x) = g(x)$$ë¥¼ ìƒì„±í•œë‹¤. 



ì—¬ê¸°ì„œ ë¬¸ì œê°€ ë‚˜ì˜¨ë‹¤. ëª¨ë“  í•¨ìˆ˜ë¥¼ ë‹¤ ì„ í˜• í•¨ìˆ˜ë¡œ ë•Œë ¤ë°•ìœ¼ë©´ ëª¨ë¸ì´ ì ì  ë³µì¡í•´ì§„ë‹¤. ì´ë ‡ê²Œ ë˜ë©´ weightì¸ $$w$$ê°€ ë§¤ìš° ì»¤ì§€ë©´ì„œ Overfittingì´ ë°œìƒí•œë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” í° weightì— í˜ë„í‹°ë¥¼ ë¶€ì—¬í•˜ëŠ” ìµíˆ ë“¤ì–´ë³¸ **Regularization**ì„ ì‚¬ìš©í•œë‹¤.

---

## Regularization & Ridge Regression

ì—¬ê¸°ì„œ Ridge, Lasso, L1, L2 ë“±ì´ ë“±ì¥í•œë‹¤.

### Ridge Regression Loss & Solution

ìˆ˜ì‹ì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
E_{ridge}(w) = \frac{1}{2} \sum_{i=1}^N (w^T \phi(x_i) - y_i)^2 + \frac{\lambda}{2} ||w||^2_2
$$

ì—¬ê¸°ì„œ $$\lambda$$ëŠ” ì–¼ë§ˆë‚˜ í˜ë„í‹°ë¥¼ ê°•í•˜ê²Œ ë¶€ì—¬í•  ê²ƒì¸ê°€ì˜ íŒŒë¼ë¯¸í„°, ì¦‰ **Regularization strength**ì´ë‹¤.

ì´ì— ëŒ€í•œ ìµœì í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. (ì´ ìµœì í•´ì˜ í˜•ì‹ë„ ê²°êµ­ ì•ì„ ë“¤ê³¼ ë¹„ìŠ·í•¨):

$$
w^{*} = (\Phi^T \Phi + \lambda I)^{-1} \Phi^T y
$$

$$\lambda I$$ê°€ ì¶”ê°€ëœ ê²ƒì´ íŠ¹ì§•ì¸ë°, ë°ì´í„° ìƒ˜í”Œ ìˆ˜ $$N$$ì´ ê¸°ì € í•¨ìˆ˜ì˜ ìˆ˜ $$M$$ë³´ë‹¤ ì ì„ ë•Œ ë°œìƒí•˜ëŠ” **Singularity** ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. Singularity ë¬¸ì œëŠ”, non-invertible, ì¦‰ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì´ìŠˆë¥¼ ëœ»í•œë‹¤.

> ğŸ’¡ **ì°¸ê³ **: Ridge regressionì€ íƒ€ê²Ÿ ë²¡í„°ì— 0ì„, Design matrixì— $$\sqrt{\lambda} I$$ë¥¼ ì¶”ê°€í•œ ì¼ë°˜ Least squares ë¬¸ì œì™€ ìˆ˜í•™ì ìœ¼ë¡œ ë™ì¼í•˜ë‹¤. 



## Bias-Variance Tradeoff

ëª¨ë¸ì˜ ì˜ˆì¸¡ ì—ëŸ¬ëŠ” ë‘ ê°€ì§€ ìš”ì†Œë¡œ ë¶„í•´ëœë‹¤: **Bias**, **Variance**

### Bias

ëª¨ë¸ì˜ ê°€ì •ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë°œìƒí•˜ëŠ” ì—ëŸ¬ (=Underfitting)ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 3ì°¨ ë‹¤í•­ì‹ ë°ì´í„°ë¥¼ ì„ í˜• íšŒê·€ë¡œ í•™ìŠµí•˜ë©´:

![Underfitting Example](/assets/img/posts/study-ml/underfitting.png)

ìœ„ì˜ ì´ë¯¸ì§€ì²˜ëŸ¼ í•™ìŠµì´ ì˜ ì´ë£¨ì–´ì§€ì§€ ì•Šìœ¼ë©° ì´ê²ƒì„ **Underfitting**ì´ë¼ê³  í•¨.

### Variance

ëª¨ë¸ì´ ë„ˆë¬´ ìœ ì—°í•´ì„œ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆê¹Œì§€ í•™ìŠµí•˜ë©° ë°œìƒí•˜ëŠ” ì´ìŠˆì´ë‹¤. **Overfitting**ì´ë¼ê³  í•˜ê³ , ì˜ˆë¥¼ ë“¤ì–´ 3ì°¨ ë‹¤í•­ì‹ ë°ì´í„°ë¥¼ 10ì°¨ ë‹¤í•­ì‹ìœ¼ë¡œ í•™ìŠµí•˜ë©´:

![Overfitting Example](/assets/img/posts/study-ml/overfitting.png)

ìœ„ì˜ ì´ë¯¸ì§€ì²˜ëŸ¼ ë„ˆë¬´ ì˜ ë§ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.



### ì „ëµ

ê·¸ëŸ¼ ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì·¨í•´ì•¼ í•  ì „ëµì€ ë¬´ì—‡ì¸ê°€?

ì¼ë°˜ì ìœ¼ë¡œ **High capacity Model**ì„ ì„ íƒí•˜ë˜, **Regularization**ì„ í†µí•´ **Variance**ë¥¼ ì¡°ì ˆí•˜ëŠ” ë°©ì‹ì„ ì·¨í•œë‹¤. ê·¸ë¦¬ê³  ëª¨ë¸ì˜ ë³µì¡ë„ $$M$$ë¥¼ ì„ íƒí•  ë•ŒëŠ” Training setì´ ì•„ë‹Œ **Validation set**ì—ì„œì˜ ì—ëŸ¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼ í•œë‹¤. 



## Probabilistic Linear Regression

ì´ëŸ° Regression ë¬¸ì œë¥¼ ì´ì „ ì±•í„°ì—ì„œ ë°°ì› ë˜ [í™•ë¥  ëª¨ë¸](/posts/Probabilistic-inference/)ë¡œ ê³µì‹í™”í•´ì„œ ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì˜ ì˜ë¯¸ë¥¼ ë‹¤ì‹œ í’€ì–´ë³´ì.

### Maximum Likelihood Estimation (MLE)

- **Likelihood** $$p(y \mid X, w, \beta)$$ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ Least squares errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ ìˆ˜í•™ì ìœ¼ë¡œ ë™ì¼í•˜ë‹¤
- **Noise precision** $$\beta$$ì˜ MLE ì¶”ì •ì¹˜ëŠ” ì˜ˆì¸¡ ì˜¤ì°¨ì˜ í‰ê· ì˜ ì—­ìˆ˜ì™€ ê°™ë‹¤

### Maximum a Posteriori (MAP)

- Weight $$w$$ì— **Zero-mean Gaussian Prior** $$p(w \mid \alpha) = \mathcal{N}(w \mid 0, \alpha^{-1} I)$$ë¥¼ ê°€ì •í•œë‹¤
- MAP Estimationì€ **Ridge regression**ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë©°, ì´ë•Œì˜ Regularization strengthëŠ” $$\lambda = \frac{\alpha}{\beta}$$ê°€ ëœë‹¤

### Fully Bayesian Linear Regression

- Weightì˜ ì  ì¶”ì •ì¹˜ê°€ ì•„ë‹Œ **Posterior distribution** $$p(w \mid D) = \mathcal{N}(w \mid \mu, \Sigma)$$ ì „ì²´ë¥¼ êµ¬í•œë‹¤
- **Posterior Predictive Distribution**: ìƒˆë¡œìš´ ë°ì´í„° $$x_{new}$$ì— ëŒ€í•´ ì˜ˆì¸¡í•  ë•Œ, íŒŒë¼ë¯¸í„° $$w$$ì˜ ë¶ˆí™•ì‹¤ì„±ì„ integrateí•œë‹¤. ì¦‰ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´:

$$
p(\hat{y}_{new} \mid x_{new}, D) = \mathcal{N}(\hat{y}_{new} \mid \mu^T \phi(x_{new}), \beta^{-1} + \phi(x_{new})^T \Sigma \phi(x_{new}))
$$

- ì´ ë°©ì‹ì€ ì…ë ¥ $$x_{new}$$ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ì˜ˆì¸¡ì˜ **Uncertainty**ë¥¼ ì •í™•í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤



---

## ê¸°íƒ€

### Weighted Least Squares

ê° ë°ì´í„° í¬ì¸íŠ¸ì— weight $$t_i$$ê°€ ë¶€ì—¬ëœ ê²½ìš°, ì´ëŠ” ê° í¬ì¸íŠ¸ì˜ **Noise variance**ê°€ $$\frac{1}{t_i}$$ë¡œ, ì„œë¡œ ë‹¤ë¥´ë‹¤ê³  ëª¨ë¸ë§í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.

### Normal-Gamma Prior

ë§Œì•½ í‰ê·  $$w$$ì™€ ì •ë°€ë„ $$\beta$$ë¥¼ ëª¨ë‘ ëª¨ë¥¼ ê²½ìš°, ì´ ë‘˜ì— ëŒ€í•œ Conjugate priorë¡œ **Normal-gamma distribution**ì„ ì‚¬ìš©í•œë‹¤.

---

## ê²°ë¡ 

ìš”ì•½í•˜ìë©´ Linear Regressionì€ ë‹¨ìˆœíˆ ì„ ì„ ê¸‹ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í™•ë¥ ì ìœ¼ë¡œëŠ” ë…¸ì´ì¦ˆê°€ ì„ì¸ ë°ì´í„°ì˜ Likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê³¼ì •ì´ê³ , Regularizationì€ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ Prior ì§€ì‹ì„ ë„ì…í•´ì„œ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•˜ëŠ” ë„êµ¬ì´ë‹¤.

---
