---
title: Linear Classification
description: Summary of linear classification, bayes' theorem, sigmoid, Hyperplane
date: 2025-12-29
categories: [Study, Machine Learning]
tags: [Machine Learning, Linear Classification, Hyperplane, Sigmoid, Naive Bayes, Bayes' Theorem]
# pin: true
math: true
mermaid: true
comments: true
image: 
    path: /assets/img/posts/study-ml/perceptron.png
    lqip: 
    alt: 
---

# Linear Classification

## Intro 2 Classification

Classificationì€ ì…ë ¥ê°’ $$x$$ë¥¼ $$C$$ê°œì˜ ì •í•´ì§„ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì´ë‹¤. (vs. Regression)

$$
y \in \{1,\ldots,C\}
$$

### Zero-one loss

ì˜ˆì¸¡ê°’ $$\hat{y}$$ì˜ í’ˆì§ˆì„ ì¸¡ì •í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ ë‹¨ìœ„ë¡œ, ë¶„ë¥˜ê°€ ì˜ëª»ëœ ìƒ˜í”Œì˜ ê°œìˆ˜ë¥¼ ì„¼ë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
\ell_{01}(y, \hat{y}) = \sum_{i=1}^N I(\hat{y} \neq y_i)
$$

### Decision Boundary (Hyperplane)

ë‘ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ” **ê²°ì • ê²½ê³„**, **Hyperplane**ìœ¼ë¡œ ì •ì˜ë˜ê³ , íŒŒë¼ë¯¸í„° $$w$$, $$w_0$$ì— ì˜í•´ $$w^T x + w_0 = 0$$ì¸ ì§€ì ì´ë‹¤.

### Perceptron Algorithm

ê°€ì¥ ì˜¤ë˜ëœ ì´ì§„ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ë¡œ, ì˜¤ë¶„ë¥˜ëœ ìƒ˜í”Œ $$x_i$$ì— ëŒ€í•´ $$w$$ì™€ $$w_0$$ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•´ì„œ ì„ í˜• ë¶„ë¦¬ê°€ ê°€ëŠ¥í•œ ê²½ìš° ë°˜ë“œì‹œ ìˆ˜ë ´í•œë‹¤.

![Perceptron Algorithm](/assets/img/posts/study-ml/perceptron.png) 

### Multiclass Strategies

- **One-versus-Rest**: ê° í´ë˜ìŠ¤ë§ˆë‹¤ í•˜ë‚˜ì˜ Hyperplaneì„ í•™ìŠµì‹œí‚¨ë‹¤
- **One-versus-One**: í´ë˜ìŠ¤ ìŒë§ˆë‹¤ Hyperplaneì„ í•™ìŠµì‹œí‚¤ê³ , Majority vote(ë‹¤ìˆ˜ê²°)ë¡œ ê²°ì •í•œë‹¤
- **Multiclass discriminant**: $$C$$ê°œì˜ ì„ í˜• í•¨ìˆ˜ $$f_c(x) = w_c^T x + w_{0_c}$$ ì¤‘ ê°€ì¥ í° ê°’ì„ ë‚´ëŠ” í´ë˜ìŠ¤ë¥¼ ì„ íƒí•œë‹¤



---

## Probabilistic Generative Models

ë°ì´í„°ì˜ ìƒì„± ê³¼ì •ì„ ëª¨ë¸ë§í•´ì„œ **Bayes' theorem**ì„ í†µí•´ **posterior probability** $p(y=c \mid x)$ë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹í™” ê°€ëŠ¥í•˜ë‹¤:

$$
p(y=c \mid x) \propto p(x \mid y=c)p(y=c)
$$

ì—¬ê¸°ì„œ $$p(x \mid y=c)$$ì™€ $$p(y=c)$$ì˜ ì˜ë¯¸ë¥¼ ë¶„ì„í•´ë³´ì.

- **Class Prior** $$p(y=c)$$ëŠ” Categorical distributionì„ ë”°ë¥´ê³ , MLE ê²°ê³¼ëŠ” ê° í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì´ $$\pi_c = \frac{N_c}{N}$$ì´ë‹¤
- **Class Conditional** $$p(x \mid y=c)$$ëŠ”, ì—°ì†í˜• ë°ì´í„°ì˜ ê²½ìš° **Multivariate Normal** $$\mathcal{N}(x \mid \mu_c, \Sigma)$$ë¥¼ ì‚¬ìš©í•œë‹¤

### Linear Discriminant Analysis (LDA)

> ğŸ’¡ **ê°€ì •**: ëª¨ë“  í´ë˜ìŠ¤ê°€ ë™ì¼í•œ Covariance Matrix $$\Sigma$$ë¥¼ ê³µìœ í•œë‹¤ê³  ê°€ì •í•´ë³´ì.

ê·¸ë ‡ë‹¤ë©´ MLEëŠ” ì–´ë–»ê²Œ ë ê¹Œ?

- $$\mu_c$$ëŠ” í´ë˜ìŠ¤ $$c$$ì— ì†í•œ ìƒ˜í”Œë“¤ì˜ í‰ê· ì´ë‹¤
- $$\Sigma$$ëŠ” ê° í´ë˜ìŠ¤ í‘œë³¸ ê³µë¶„ì‚° $$S_c$$ì˜ ê°€ì¤‘ í‰ê· ì´ë‹¤

ì—¬ê¸°ì„œ Logì˜ í™•ë¥  ë¹„ìœ¨ì„ ê³„ì‚°í•˜ë©´ $$x$$ì— ëŒ€í•œ ì„ í˜•ì‹ $$w^T x + w_0$$ì´ ë˜ê³ , ì´ê²ƒì€ **Linear decision boundary**ë¥¼ ì´ë£¬ë‹¤.

![Linear Discriminant Analysis](/assets/img/posts/study-ml/lda.png)

### Naive Bayes

> ğŸ’¡ **ê°€ì •**: í´ë˜ìŠ¤ê°€ ì£¼ì–´ì¡Œì„ ë•Œ featuresë“¤ì´ ì„œë¡œ independentë¼ê³  ê°€ì •í•´ë³´ì.

ì´ ê²½ìš° Covariance matrixëŠ” Diagonal matrixê°€ ë˜ë©°, í´ë˜ìŠ¤ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ $$\Sigma_c$$ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ë¦¬ê³  ì´ $$\Sigma_c$$ê°€ ì„œë¡œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, $$x$$ì— ëŒ€í•œ 2ì°¨í•­ì´ ì‚´ì•„ë‚¨ì•„ **Quadratic decision boundary**ë¥¼ í˜•ì„±í•œë‹¤.

![Naive Bayes](/assets/img/posts/study-ml/naive_bayes.png)




---

## Probabilistic Discriminative Models

ì´ë²ˆì—” **ì‚¬í›„ í™•ë¥ ** $$p(y \mid x)$$ë¥¼ ì§ì ‘ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì´ë‹¤. ëŒ€ëµ 3ê°€ì§€ ì •ë„ì˜ ë°©ë²•ì´ ìˆëŠ”ë°, **Logistic Regression**, **Multiclass Logistic Regression**, **Weight Regularization** ì •ë„ê°€ ìˆë‹¤.

### Logistic Regression

Binary classificationì„ $$p(y=1 \mid x) = \sigma(w^T x)$$ë¡œ ëª¨ë¸ë§í•œë‹¤. ì—¬ê¸°ì„œ $$\sigma(a) = \frac{1}{1+e^{-a}}$$ëŠ” **Sigmoid function**ì´ë‹¤.

![Sigmoid Function](/assets/img/posts/study-ml/sigmoid.png)

- **Negative Log-Likelihood**: Loss Functionìœ¼ë¡œëŠ” **Binary Cross Entropy**ë¥¼ ì‚¬ìš©í•œë‹¤
- **Optimization**: Closed-form solutionì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ìµœì í™”ê°€ í•„ìš”í•˜ë‹¤

### Multiclass Logistic Regression

**Softmax function**ì„ ì‚¬ìš©í•´ì„œ ì—¬ëŸ¬ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤:

$$
p(y=c \mid x) = \frac{\exp(w_c^T x)}{\sum_{c'} \exp(w_{c'}^T x)}
$$

![Softmax Function](/assets/img/posts/study-ml/softmax.png)

- **Loss Function**ì€ **Cross Entropy**ë¥¼ ì‚¬ìš©í•˜ë©°, íƒ€ê²Ÿì€ **One-hot encoding**ìœ¼ë¡œ í‘œí˜„ëœë‹¤

### Weight Regularization

**Overfittingì„ ë°©ì§€**í•˜ê¸° ìœ„í•´ í˜ë„í‹°ë¥¼ ì¶”ê°€í•˜ëŠ” ì „ëµ. í˜ë„í‹°ì˜ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
E(w) + \lambda ||w||_q^q
$$


---

## Key Insights

### 1. Limitation of MLE (Linearly Separable Data)

ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ ì™„ë²½íˆ ë¶„ë¦¬ë  ê²½ìš°, MLEë¥¼ í†µí•œ $$w$$ì˜ í¬ê¸°ê°€ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ê²Œ ëœë‹¤. $$ \| w \| \to \infty$$ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Weight regularization**ì€ í•„ìˆ˜ì ì´ë‹¤.

### 2. Differentiation of Sigmoid

$$
\frac{d\sigma(a)}{da} = \sigma(a)(1 - \sigma(a))
$$

### 3. Identifiability

$$C$$ê°œ í´ë˜ìŠ¤ì— ëŒ€í•´ $$C$$ê°œì˜ íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” Softmax ëª¨ë¸ì€ ì¤‘ë³µì„±ì´ ìˆë‹¤. ë”°ë¼ì„œ í•œ í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ê³ ì •í•˜ëŠ” ì œì•½ì„ ë‘ê¸°ë„ í•˜ëŠ”ë°, ì´ì§„ ë¶„ë¥˜ì—ì„œ Sigmoidë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ë¡œ ì´ ì œì•½ì´ í¬í•¨ëœ í˜•íƒœì´ë‹¤.

### 4. Generative vs. Discriminative

ì¼ë°˜ì ìœ¼ë¡œ íŒë³„ ëª¨ë¸ì´ ë¶„ë¥˜ ì„±ëŠ¥ì´ ë” ë›°ì–´ë‚˜ì§€ë§Œ, ìƒì„± ëª¨ë¸ì€ ëˆ„ë½ëœ ë°ì´í„° ì²˜ë¦¬ë‚˜ ì•„ì›ƒë¼ì´ì–´ íƒì§€ì— ë” ìœ ë¦¬í•˜ë‹¤.



---

## ê²°ë¡ 

**Linear Classification**ì€ Decision Boundaryë¥¼ ì§ì„  ë˜ëŠ” Hyperplaneìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê³ , ì´ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì ‘ê·¼í•  ë•Œ **Generative** (í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬ë¥¼ ê°€ì •) ë˜ëŠ” **Discriminative** (ê²½ê³„ ìì²´ë¥¼ ì§ì ‘ í•™ìŠµ)ì— ë”°ë¼ LDAì™€ Logistic Regression ë“±ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.

---
